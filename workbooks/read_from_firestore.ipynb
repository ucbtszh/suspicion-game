{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE: load trial info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/Sarah Zheng/dev/cards-deception/cards-game/src/assets/trials_40shuffled.json\", \"r\") as read_file:\n",
    "    trials_source = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = pd.DataFrame(trials_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add expectation violation levels\n",
    "\n",
    "trials['exp_violation'] = np.where(trials['outcome'] == -1, trials['outcome'] - trials['outcome'] * (trials['n_red']/5), \\\n",
    "                                   trials['outcome'] - trials['outcome'] * (5-trials['n_red'])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['n_blue'] = 5-trials['n_red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_red</th>\n",
       "      <th>outcome</th>\n",
       "      <th>exp_violation</th>\n",
       "      <th>n_blue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_red  outcome  exp_violation  n_blue\n",
       "0      1       -1           -0.8       4\n",
       "1      3       -1           -0.4       2\n",
       "2      2        1            0.4       3\n",
       "3      1       -1           -0.8       4\n",
       "4      2       -1           -0.6       3"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=trials.exp_violation, legend=False).set(xlabel=\"trial\", ylabel=\"expectation violation at each trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial-to-trial change in expectation violation\n",
    "exp_violation_gradient = trials.exp_violation.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=exp_violation_gradient, legend=False).set(xlabel=\"trial\", ylabel=\"change in expectation violation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative sum of expectation violation over trials\n",
    "sns.lineplot(data=trials.exp_violation.cumsum(), legend=False).set(xlabel=\"trial\", ylabel=\"cumulative range in expectation violation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks for each level of expectation violation\n",
    "mask_min08 = trials[trials.exp_violation == '-0.8'].index.tolist()\n",
    "mask_min06 = trials[trials.exp_violation == '-0.6'].index.tolist()\n",
    "mask_min04 = trials[trials.exp_violation == '-0.4'].index.tolist()\n",
    "mask_min02 = trials[trials.exp_violation == '-0.2'].index.tolist()\n",
    "\n",
    "mask_08 = trials[trials.exp_violation == '0.8'].index.tolist()\n",
    "mask_06 = trials[trials.exp_violation == '0.6'].index.tolist()\n",
    "mask_04 = trials[trials.exp_violation == '0.4'].index.tolist()\n",
    "mask_02 = trials[trials.exp_violation == '0.2'].index.tolist()\n",
    "\n",
    "masks = [mask_min08, mask_min06, mask_min04, mask_min02, mask_02, mask_04, mask_06, mask_08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexes of trials by # red cards = states\n",
    "idx_n_red = []\n",
    "\n",
    "for i in set(trials.n_red):\n",
    "    idxs = trials[trials.n_red == i].index.tolist()\n",
    "    print(\"#red cards:\", i, \"at\", idxs)\n",
    "    idx_n_red.append(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get pilot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pilot uuids\n",
    "uuids = ['5fe200ac3803bbcff9845530', \n",
    "         '5ff5c230bce45b062a2a06b2', \n",
    "         '5f1b05a8703c380d1393bb27', \n",
    "         '5b424267e8815c0001777d94', \n",
    "         '604294f7bacd9a1a46cf4da5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "\n",
    "# Use a service account\n",
    "cred = credentials.Certificate('D:/Behavioural cybersecurity/project-notes/cards-dd-game-61b9b7f26a6e.json')\n",
    "firebase_admin.initialize_app(cred)\n",
    "\n",
    "db = firestore.client()\n",
    "\n",
    "subjects_ref = db.collection(u'subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses query reference\n",
    "refs = []\n",
    "for uuid in uuids:\n",
    "    refs.append(subjects_ref.document(uuid).collection(u'responses').document('main_responses').get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographics survey query reference\n",
    "refs_demographics = []\n",
    "for uuid in uuids:\n",
    "    refs_demographics.append(subjects_ref.document(uuid).collection(u'responses').document('demographics').get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# card values\n",
    "transdict = {'blue': 1, 'red': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# literal card values\n",
    "transdict2 = {1: 'blue', -1: 'red'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result values\n",
    "results_transdict = {'tie': 0.5, 'loss': 1, 'win': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch responses from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameResponses(object):\n",
    "    def __init__(self, randomPick, randomPickColour, reportColour, RTreport, honestyRating, RThonesty,\n",
    "                 results, catchRating, RTcatch):\n",
    "        self.randomPick = randomPick\n",
    "        self.randomPickColour = randomPickColour\n",
    "        self.reportColour = reportColour\n",
    "        self.honestyRating = honestyRating\n",
    "        self.catchRating = catchRating\n",
    "        self.RThonesty = RThonesty\n",
    "        self.RTreport = RTreport\n",
    "        self.RTcatch = RTcatch\n",
    "        self.results = results\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(source):\n",
    "        gresponse = GameResponses(source[u'randomPick'], source[u'randomPickColour'], source[u'reportColour'],\n",
    "                                  source[u'RTreport'], source[u'honestyRating'], source[u'RThonesty'], \n",
    "                                  source[u'results'], source[u'catchRating'], source[u'RTcatch'])\n",
    "\n",
    "        return gresponse\n",
    "\n",
    "    def to_dict(self):\n",
    "        dest = {\n",
    "            u'randomPick': self.randomPick,\n",
    "            u'randomPickColour': self.randomPickColour,\n",
    "            u'reportColour': self.reportColour,\n",
    "            u'RTreport': self.RTreport,\n",
    "            u'honestyRating': self.honestyRating,\n",
    "            u'RThonesty': self.RThonesty,\n",
    "            u'results': self.results,\n",
    "            u'catchRating': self.catchRating,\n",
    "            u'RTcatch': self.catch\n",
    "        }\n",
    "\n",
    "        return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Demographics(object):\n",
    "    def __init__(self, age, catch, edlev, gender, twin):\n",
    "        self.age = age\n",
    "        self.catch = catch\n",
    "        self.edlev = edlev\n",
    "        self.gender = gender\n",
    "        self.twin = twin\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(source):\n",
    "        demos = Demographics(source[u'age'], source[u'catch'], source[u'edlev'],\n",
    "                                  source[u'gender'], source[u'twin'])\n",
    "\n",
    "        return demos\n",
    "\n",
    "    def to_dict(self):\n",
    "        dest = {\n",
    "            u'age': self.age,\n",
    "            u'catch': self.catch,\n",
    "            u'edlev': self.edlev,\n",
    "            u'gender': self.gender,\n",
    "            u'twin': self.twin\n",
    "        }\n",
    "\n",
    "        return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main game response data\n",
    "honestyRatings = []\n",
    "catchRating = []\n",
    "reportHonesty = []\n",
    "gameResults = []\n",
    "transGameResults = []\n",
    "RThonesty = []\n",
    "RTreport = []\n",
    "randomPickColour = []\n",
    "reportColour = []\n",
    "\n",
    "for index, subject in enumerate(refs):\n",
    "    responses = GameResponses.from_dict(subject.to_dict())\n",
    "    \n",
    "    ratings = [int(numeric_string) for numeric_string in responses.honestyRating]\n",
    "    honestyRatings.append(ratings)\n",
    "    \n",
    "    catch = [int(numeric_string) for numeric_string in responses.catchRating]\n",
    "    catchRating.append(catch)\n",
    "    \n",
    "    ifLied = np.array([transdict[x] for x in responses.randomPickColour]) == responses.reportColour\n",
    "    reportHonesty.append(ifLied)\n",
    "    \n",
    "    gameResults.append(responses.results)\n",
    "    \n",
    "    transResults = np.array([results_transdict[x] for x in responses.results]) \n",
    "    transGameResults.append(transResults)\n",
    "    \n",
    "    RThonesty.append(responses.RThonesty)\n",
    "    RTreport.append(responses.RTreport)\n",
    "    \n",
    "    randomPickColour.append([x for x in responses.randomPickColour])\n",
    "    reportColour.append([x for x in responses.reportColour])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check when subjects lied\n",
    "\n",
    "lying_overview = []\n",
    "lying_counts = []\n",
    "lying_probable = []\n",
    "\n",
    "for i in range(len(uuids)):\n",
    "    lie = 1 - reportHonesty[i]\n",
    "    tmp_sheet = pd.DataFrame({'random_pick_colour': [transdict[x] for x in randomPickColour[i]], \n",
    "                              'reported_colour': reportColour[i], \n",
    "                              'lied': lie})\n",
    "    tmp_sheet = trials.join(tmp_sheet)\n",
    "    tmp_sheet['lied_colour_probability'] = np.where(tmp_sheet['reported_colour']==-1, np.round(tmp_sheet['n_red']/tmp_sheet['n_blue'], 2), np.round(tmp_sheet['n_blue']/tmp_sheet['n_red'], 2))\n",
    "    tmp_sheet['lied_colour_probability'] = np.where(tmp_sheet['lied']==1, tmp_sheet['lied_colour_probability'], 0)\n",
    "    lying_overview.append(tmp_sheet)\n",
    "    \n",
    "    lie_blue = sum((tmp_sheet.lied == 1) & (tmp_sheet.reported_colour == 1))\n",
    "    lie_red = sum((tmp_sheet.lied == 1) & (tmp_sheet.reported_colour == -1))\n",
    "    lying_counts.append({'count_lie_blue': lie_blue, 'count_lie_red': lie_red, 'average_honesty_rating': np.array(honestyRatings[i]).mean()})\n",
    "    \n",
    "    lying_probable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "lying_counts = pd.DataFrame(lying_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_red</th>\n",
       "      <th>outcome</th>\n",
       "      <th>exp_violation</th>\n",
       "      <th>n_blue</th>\n",
       "      <th>random_pick_colour</th>\n",
       "      <th>reported_colour</th>\n",
       "      <th>lied</th>\n",
       "      <th>lied_colour_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_red  outcome  exp_violation  n_blue  random_pick_colour  reported_colour  \\\n",
       "0      1       -1           -0.8       4                   1               -1   \n",
       "1      3       -1           -0.4       2                  -1                1   \n",
       "2      2        1            0.4       3                   1               -1   \n",
       "3      1       -1           -0.8       4                  -1               -1   \n",
       "4      2       -1           -0.6       3                  -1                1   \n",
       "\n",
       "   lied  lied_colour_probability  \n",
       "0     1                     0.25  \n",
       "1     1                     0.67  \n",
       "2     1                     0.67  \n",
       "3     0                     0.00  \n",
       "4     1                     1.50  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lying_overview[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x23cbfe66fd0>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 438.5x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(data=lying_overview[3], x='lied_colour_probability', hue='reported_colour', bins=35, multiple='stack').set(xlim=(0.2,2), ylim=(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, pearsonr, ttest_ind, wilcoxon, ranksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7891848733011686, 0.11244825052684987)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(lying_counts['average_honesty_rating'], lying_counts['count_lie_blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6149967904317033, 0.2695901711383851)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(lying_counts['average_honesty_rating'], lying_counts['count_lie_red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.0023226118829302155, pvalue=0.9886515231599244)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(lying_overview[0].lied_colour_probability, lying_overview[0].random_pick_colour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=-0.22722002748358947, pvalue=0.15853468944885488)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(lying_overview[3].lied_colour_probability, lying_overview[0].random_pick_colour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5892580080066796"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lying_overview[3].lied_colour_probability[lambda x: x >0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6675"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.67/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_lie_blue</th>\n",
       "      <th>count_lie_red</th>\n",
       "      <th>average_honesty_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>3.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_lie_blue  count_lie_red  average_honesty_rating\n",
       "0              10             12                   3.175\n",
       "1              12              2                   3.225\n",
       "2               0              0                   5.325\n",
       "3               2              2                   3.425\n",
       "4               1              0                   4.700"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lying_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=0.4177863742936748, pvalue=0.6761033140231469)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(lying_counts.count_lie_blue, lying_counts.count_lie_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "writer = pd.ExcelWriter('pilot1_lying_per_subject.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for i, df in enumerate(lying_overview):\n",
    "    df.to_excel(writer, sheet_name='subject'+str(i))\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing honesty ratings -> suspicion ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [honesty rating] normalize honesty ratings to range [0;1] and reverse-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_honesty_ratings = []\n",
    "\n",
    "for ratings in honestyRatings:\n",
    "    normed = [(v/6) for v in ratings]\n",
    "    normalized_honesty_ratings.append(normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse-code honesty ratings to reflect suspicion\n",
    "import numexpr\n",
    "suspicionRating = numexpr.evaluate('(6 - honestyRatings)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_suspicion_ratings = numexpr.evaluate('(1 - normalized_honesty_ratings)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_suspicion_ratings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [honesty rating] binarize reversed-coded honesty ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_suspicion_ratings = []\n",
    "\n",
    "for ratings in suspicionRating:\n",
    "    binary = [1 if v >= 4 else 0 for v in ratings]\n",
    "    binary_suspicion_ratings.append(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_suspicion_ratings[1][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [honesty rating] z-scored raw reverse-coded honesty ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 1: get pooled mean and SD stats from all participants\n",
    "bin_mean_HRs = []\n",
    "bin_sd_HRs = []\n",
    "\n",
    "for rating in suspicionRating:\n",
    "    bin_mean_HRs.append(sum(rating)/len(rating))\n",
    "    bin_sd_HRs.append(np.array(rating).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean_HRs = np.array(bin_mean_HRs).mean()\n",
    "z_sd_HRs = np.array(bin_mean_HRs).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-scored suspicion ratings compared cross-participants\n",
    "z_suspicion_ratings = []\n",
    "\n",
    "for rating in suspicionRating:\n",
    "    z_rating = [(x-z_mean_HRs)/z_sd_HRs for x in rating]\n",
    "    z_suspicion_ratings.append(z_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sd_HRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 2: z-score each participant's ratings -> takes away inter-subject variance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [honesty rating] log transform normalized reverse-coded honesty ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_log_suspicionrating = [np.log(v+1) for v in normalized_suspicion_ratings[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=tmp_log_suspicionrating, legend=False).set(xlabel=\"trial\", ylabel=\"log normalized honesty rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial inspection plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot honesty ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(honestyRatings)):\n",
    "    sns.lineplot(data=honestyRatings[i], legend=False).set(xlabel=\"trial\", ylabel=\"honesty rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(honestyRatings)):\n",
    "    sns.lineplot(data=normalized_suspicion_ratings[i], legend=False).set(xlabel=\"trial\", ylabel=\"suspicion rating (reversed honesty)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject that lied most\n",
    "sns.lineplot(data=honestyRatings[0], legend=False)\n",
    "sns.lineplot(data=gameResults[0], legend=False).set(xlabel=\"trial\", ylabel=\"game result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject that lied most\n",
    "sns.lineplot(data=honestyRatings[0], legend=False)\n",
    "sns.lineplot(data=trials.exp_violation, legend=False).set(xlabel=\"trial\", ylabel=\"expectation violation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_tmp = exp_violation_gradient.cumsum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject that lied most\n",
    "sns.lineplot(data=honestyRatings[0], legend=False)\n",
    "sns.lineplot(data=ev_tmp, legend=False).set(xlabel=\"trial\", ylabel=\"cumulative expectation violation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject that lied most\n",
    "sns.lineplot(data=honestyRatings[0], legend=False)\n",
    "sns.lineplot(data=exp_violation_gradient, legend=False).set(xlabel=\"trial\", ylabel=\"change in expectation violation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# subject that lied least\n",
    "sns.lineplot(data=honestyRatings[2], legend=True)\n",
    "sns.lineplot(data=gameResults[2], legend=True).set(xlabel=\"trial\", ylabel=\"game result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save specific subject data to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_subject(i):\n",
    "    dict_subject = {\n",
    "        \"trial\": range(1,41),\n",
    "        \"n_red\": trials.n_red,\n",
    "        \"n_blue\": 5-trials.n_red,\n",
    "        \"opponent_report\": trials.outcome,\n",
    "        \"expectation_violation\": trials.exp_violation,\n",
    "        \"random_pick_colour\": [transdict[x] for x in randomPickColour[i]],\n",
    "        \"reported_pick_colour\": reportColour[i],\n",
    "        \"honest_card_report\": reportHonesty[i],\n",
    "        \"trial_result\": transGameResults[i],\n",
    "        \"honesty_rating\": honestyRatings[i],\n",
    "        \"RT_honesty\": RThonesty[i],\n",
    "        \"normalized_reversed_honesty_rating\": normalized_suspicion_ratings[i]\n",
    "    }\n",
    "\n",
    "    df_subject = pd.DataFrame(dict_subject)\n",
    "    df_subject['surprise'] = abs(df_subject['expectation_violation'])\n",
    "    df_subject['plot_partner_report'] = 1.1\n",
    "    df_subject['plot_subject_report'] = -0.1\n",
    "    df_subject['plot_random_pick_colour'] = -0.15\n",
    "    df_subject['lied'] = df_subject['random_pick_colour'] != df_subject['reported_pick_colour']\n",
    "    df_subject['colour_incongruent'] = df_subject['reported_pick_colour'] != df_subject['opponent_report']\n",
    "    df_subject['colour_congruent'] = df_subject['reported_pick_colour'] == df_subject['opponent_report']\n",
    "    \n",
    "    df_subject['partner_result'] = [1 if x == 0 else 0 if x == 1 else x for x in df_subject.trial_result.values]\n",
    "    return df_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers2 = {\"loss\": \"X\", \"win\": \"o\", \"tie\": \"D\"}\n",
    "colors = {-1: 'red', 1: 'blue'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_plot_subject(df_subject, i):\n",
    "    sns.set(rc={'figure.figsize':(16,7)})\n",
    "    p = sns.lineplot(data=df_subject, x='trial', y='normalized_reversed_honesty_rating')\n",
    "    p.set(ylim=(-0.25,1.2), xlim=(0, 41))\n",
    "    p.set_ylabel('actual normalized reversed honesty ratings')\n",
    "    p.set_title('Cards game progression subject '+str(i+1))\n",
    "\n",
    "    p2 = sns.lineplot(data=df_subject, x='trial', y='surprise')\n",
    "#     p2 = sns.lineplot(data=trials, x='trial', y='normed_signed_colour_count')\n",
    "    s = sns.scatterplot(data=df_subject, x='trial',y='plot_random_pick_colour', c=[colors[x] for x in df_subject.random_pick_colour])\n",
    "    s1 = sns.scatterplot(data=df_subject, x='trial',y='plot_subject_report', c=[colors[x] for x in df_subject.reported_pick_colour])\n",
    "    s2 = sns.scatterplot(data=df_subject, x='trial',y='plot_partner_report', c=[colors[x] for x in df_subject.opponent_report])\n",
    "    s3 = sns.scatterplot(data=df_subject, x='trial',y='trial_result', markers=markers2, style=gameResults[i])\n",
    "    s3.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    s4 = sns.scatterplot(data=df_subject, x='trial', y=[-0.2 if x else None for x in df_subject.lied])\n",
    "    s5 = sns.scatterplot(data=df_subject, x='trial', y=[1.15 if x == True else None for x in df_subject.colour_congruent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "df_subject = create_df_subject(i)\n",
    "create_plot_subject(df_subject, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearmanr(df_subject.lied, df_subject.normalized_reversed_honesty_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_subject.lied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((df_subject.random_pick_colour == -1) & (df_subject.reported_pick_colour == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(df_subject.colour_incongruent, df_subject.normalized_reversed_honesty_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(abs(df_subject.expectation_violation), df_subject.normalized_reversed_honesty_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(abs(df_subject.expectation_violation), df_subject.lied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(df_subject.expectation_violation, df_subject.lied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(df_subject.normalized_reversed_honesty_rating, df_subject.lied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subject.to_csv('pilot_subject1.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export honesty ratings to .csv file\n",
    "# import csv\n",
    "# with open('pilot_honestyratings.csv', 'w') as csvfile:\n",
    "#     csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "#     csvwriter.writerows(honestyRatings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [CONTROL] attention check questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographics data\n",
    "edlev = []\n",
    "catch = []\n",
    "\n",
    "for subject in refs_demographics:\n",
    "    demos = Demographics.from_dict(subject.to_dict())\n",
    "    edlev.append(demos.edlev)\n",
    "    catch.append(demos.catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-game attention checks\n",
    "catchRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_feedback = []\n",
    "\n",
    "for uuid in uuids:\n",
    "    refs_feedback.append(subjects_ref.document(uuid).collection(u'responses').document('feedback').get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feedback in refs_feedback:\n",
    "    print(feedback.to_dict()['studyPurpose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RT honesty ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RThonesty_stats = pd.DataFrame(columns=['mean', 'sd', 'min', 'max'])\n",
    "\n",
    "for id in RThonesty:\n",
    "    mean = sum(id)/len(id)\n",
    "    sd = np.array(id).std()\n",
    "    vmin = min(id)\n",
    "    vmax = max(id)\n",
    "    tmp = {'mean': mean, 'sd': sd, 'min': vmin, 'max': vmax}\n",
    "    print(tmp)\n",
    "    RThonesty_stats.append(tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter estimation - grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import *\n",
    "from specs import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process trials\n",
    "from specs import Trial\n",
    "\n",
    "Trials = []\n",
    "for trial in trials_source:\n",
    "    Trials.append(Trial(n_red=trial['n_red'], outcome=trial['outcome']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [model 1] using unsigned suspicion formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid space\n",
    "alpha = np.linspace(0.1, 1, 10)\n",
    "s0 = [-1, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 1]\n",
    "\n",
    "params = (list(product(alpha, s0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[142]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search param value fit\n",
    "best_param_sv_rating = []\n",
    "all_residuals_sv_rating = []\n",
    "all_R2_sv_rating = []\n",
    "best_param_sv_rating_SSE = []\n",
    "best_param_sv_rating_R2 = []\n",
    "sklearn_r2 = []\n",
    "\n",
    "for p, data in enumerate(normalized_suspicion_ratings):\n",
    "    suspicion_ratings = np.array(data)\n",
    "    SStot_sv_rating = np.sum((suspicion_ratings-suspicion_ratings.mean())**2)\n",
    "    SSres_sv_rating = []\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        print(\"iterating parameter set\", i)\n",
    "        g = Game(Trials, Player(alpha=param[0], pre_suspicion=param[1]))\n",
    "        g.simulate_unsigned(verbose=False, save=False, add_noise=False)\n",
    "\n",
    "#         sim_sv_rating = g.unsigned_suspicion_to_honesty_rating()\n",
    "        sim_sv_rating = g.unsigned_expectation_violation\n",
    "        residuals_sv_rating = np.sum((suspicion_ratings-sim_sv_rating)**2)\n",
    "        SSres_sv_rating.append(residuals_sv_rating)\n",
    "        \n",
    "        sklearn_r2.append(r2_score(data, sim_sv_rating))\n",
    "        \n",
    "    R2_sv_rating = 1 - np.divide(SSres_sv_rating, SStot_sv_rating)\n",
    "    all_residuals_sv_rating.append(SSres_sv_rating)\n",
    "    all_R2_sv_rating.append(R2_sv_rating)\n",
    "    \n",
    "    best_idx_sv_rating = np.array(SSres_sv_rating).argmin()\n",
    "    best_param_sv_rating.append(params[best_idx_sv_rating])\n",
    "    best_param_sv_rating_SSE.append(np.array(SSres_sv_rating).min())\n",
    "    best_param_sv_rating_R2.append(R2_sv_rating[best_idx_sv_rating])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_sv_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(params) #* len(uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_param_sv_rating_R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(sklearn_r2[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_residuals_sv_rating[0]).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[98]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [model 2] signed suspicion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter search space\n",
    "\n",
    "alpha = np.linspace(0.01,1,100)\n",
    "s0 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_suspicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_suspicion.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = generated_suspicion - generated_suspicion.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(t ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SStot_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = SSres_sv[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSres_sv = []\n",
    "generated_suspicion = normalized_suspicion_ratings[0]\n",
    "SStot_sv = np.sum((generated_suspicion - generated_suspicion.mean())**2)\n",
    "logs = []\n",
    "\n",
    "for i, param in enumerate(alpha):\n",
    "    g = Game(Trials, Player(alpha=param, pre_suspicion=s0))\n",
    "    g.simulate_signed(verbose=False, save=False, add_noise=False)\n",
    "\n",
    "    sv = np.array(g.suspicion_values)\n",
    "    print(\"model output\", sv)\n",
    "    sv = normalized_array(sv, s0)\n",
    "    print(\"normalized model output\", sv)\n",
    "    residuals_sv = np.sum((generated_suspicion-sv)**2)\n",
    "    SSres_sv.append(residuals_sv)\n",
    "    tmp = pd.DataFrame(g.sim_log)\n",
    "    tmp['normalized_estimated_suspicion'] = g.normalize_signed_suspicion()\n",
    "    tmp['normalized_actual_suspicion_ratings_subject1'] = generated_suspicion\n",
    "    logs.append(tmp)\n",
    "    \n",
    "R2_sv = 1 - np.divide(SSres_sv, SStot_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_sv.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha[76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "writer = pd.ExcelWriter('pilot_subject1_signed-suspicion-noprior_gridsearch.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for i, df in enumerate(logs):\n",
    "    df.to_excel(writer, sheet_name='fit_alpha_'+str(alpha[i]))\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({'alpha': alpha, 'SS_res': SSres_sv, 'SS_tot': [SStot_sv]*len(alpha), 'R_squared': R2_sv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv('pilot_subject1_signed-suspicion-noprior_gridsearch_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [model 3] using softmax probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_recovery_proba(x, params):\n",
    "    '''returns sum of squared residuals for each fitted parameter value combination, and best parameter values index'''\n",
    "    SSres = []\n",
    "    for param in params:\n",
    "        player = Player(alpha=param[0], beta=param[1])\n",
    "        g = Game(trials, player)\n",
    "        g.simulate(verbose=False)\n",
    "        ps = np.array(g.softmax_probabilities)\n",
    "        SSres.append(sum((x-ps)**2))\n",
    "    minimum_idx = np.where(SSres == np.amin(SSres))[0]\n",
    "    print(\"SSresiduals minimum index:\", minimum_idx)\n",
    "    for idx in minimum_idx:\n",
    "        print(\"BEST PARAMETER ESTIMATES:\", params[idx])\n",
    "    return np.array(SSres), minimum_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_best_estimations(x, params):\n",
    "    '''generates long dataframe with estimated and actual data'''\n",
    "    x = np.array(x)\n",
    "    SSres, best_params_idx = param_recovery_proba(x, params)\n",
    "    SStot = sum([(v-x.mean())**2 for v in x])\n",
    "    R2 = 1-SSres[best_params_idx]/SStot\n",
    "    \n",
    "    best_sim = Game(trials, Player(alpha=params[best_params_idx[0]][0], beta=params[best_params_idx[0]][1]))\n",
    "    best_sim.simulate(verbose=False)\n",
    "    best_estimated_proba = best_sim.softmax_probabilities\n",
    "    \n",
    "    df = pd.DataFrame({'value': best_estimated_proba, 'label': ['estimated'] * len(best_estimated_proba)})\n",
    "    df = df.append(pd.DataFrame({'value': x, 'label': ['actual'] * len(x)}), ignore_index=False)\n",
    "    print(\"SSres\", SSres[best_params_idx])\n",
    "    print(\"SStot\", SStot)\n",
    "    print(\"R squared:\", R2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_best_estimations(normalized_suspicion_ratings[0], params)\n",
    "sns.lineplot(data=df, x=df.index, y='value', hue='label').set(xlabel=\"trial\", ylabel=\"p(deception)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_best_estimations(transGameResults[4], params)\n",
    "sns.lineplot(data=df, x=df.index, y='value', hue='label').set(xlabel=\"trial win/loss/tie\", ylabel=\"p(deception)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [model 4] fit simple colour report counts tracking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add partner colour report outcome counts to trial\n",
    "\n",
    "count_red = abs(trials.outcome[lambda x: x==-1].cumsum())\n",
    "count_blue = abs(trials.outcome[lambda x: x==1].cumsum())\n",
    "\n",
    "trials['colour_count'] = count_red.append(count_blue).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['colour_count_prop'] = trials.colour_count/len(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approach 1: track proportion of reported colours\n",
    "\n",
    "prop_trial_red = []\n",
    "prop_trial_blue = []\n",
    "\n",
    "t_burnin = 0 # how many trials in memory\n",
    "\n",
    "for i in range(len(trials)):\n",
    "    tmp = trials.outcome[:i]\n",
    "    n_red = abs(tmp[lambda x: x==-1].cumsum())\n",
    "    n_blue = abs(tmp[lambda x: x==1].cumsum())\n",
    "    prop_trial_red.append(n_red/i)\n",
    "    prop_trial_blue.append(n_blue/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_trial_red[2].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approach 2: track # same consecutive colour reports since the reported colour changed\n",
    "\n",
    "track_freq = [1] * len(trials)\n",
    "\n",
    "for i, outcome in enumerate(trials.outcome.values):\n",
    "    if (i==0):\n",
    "        continue\n",
    "    if (i>0):\n",
    "        if (outcome != trials.outcome.values[i-1]):\n",
    "            continue\n",
    "        if (outcome == trials.outcome.values[i-1]):\n",
    "            track_freq[i] = track_freq[i-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['trial'] = trials.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['n_consec_colour'] = track_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['signed_n_consec_colour'] = trials['n_consec_colour'] * trials['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (trials.signed_n_consec_colour + 5)/9\n",
    "trials['normed_signed_colour_count'] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_count_tracker(data, params):\n",
    "    data = np.array(data)\n",
    "    ss_tot = np.sum((data - data.mean())**2)\n",
    "    ss_res = []\n",
    "    for param in params:\n",
    "        alpha = param[0]\n",
    "        prior = param[1]\n",
    "        estimated = np.array(trials.normed_signed_colour_count * alpha + prior)\n",
    "        ss_res.append(np.sum((data - estimated)**2))\n",
    "    r2 = 1 - np.divide(ss_res, ss_tot)\n",
    "    best_params = params[np.array(r2).argmax()]\n",
    "    return best_params, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid space\n",
    "alpha = np.linspace(0.1, 1, 10)\n",
    "prior = [-0.2, -0.1, 0, 0.1, 0.2]\n",
    "\n",
    "params = (list(product(alpha, prior)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, r2 = fit_count_tracker(normalized_suspicion_ratings[1], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, r2 = fit_count_tracker(normalized_suspicion_ratings[4], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=trials.normed_signed_colour_count * 0.8 - 0.2)\n",
    "sns.lineplot(data=normalized_suspicion_ratings[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overall analyses across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct RT report cumulative values\n",
    "rtreport0_tmp = pd.Series(RTreport[0]).diff().fillna(RTreport[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(rtreport0_tmp, honestyRatings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average honesty rating and SD\n",
    "avgHonestyRatings = []\n",
    "sdHonestyRatings = []\n",
    "\n",
    "for index, ratings in enumerate(honestyRatings):\n",
    "#     print(index, ratings)\n",
    "    ratingsInt = [int(numeric_string) for numeric_string in ratings]\n",
    "    mean = sum(ratingsInt)/len(ratingsInt)\n",
    "    avgHonestyRatings.append(mean)\n",
    "    sd = np.std(ratingsInt)\n",
    "    sdHonestyRatings.append(sd)\n",
    "    print(\"average honesty rating\", mean, \"std\", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgHonestyRatings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdHonestyRatings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propLied = []\n",
    "\n",
    "for index, report in enumerate(reportHonesty):\n",
    "#     print(index, report)\n",
    "    lieProportion = (len(report) - sum(report)) / len(report)\n",
    "    print(lieProportion)\n",
    "    propLied.append(lieProportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate pearson's correlation proportion the participant lied and average honesty rating\n",
    "pearsonr(propLied, avgHonestyRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(propLied, avgHonestyRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate game results per participant\n",
    "wins = []\n",
    "losses = []\n",
    "ties = []\n",
    "\n",
    "for result in gameResults:\n",
    "    unique, counts = np.unique(result, return_counts=True)\n",
    "    outcome = dict(zip(unique, counts))\n",
    "    wins.append(outcome['win'])\n",
    "    losses.append(outcome['loss'])\n",
    "    ties.append(outcome['tie'])\n",
    "#     print(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate spearman's correlation game results (win/loss/tie) & average honesty rating\n",
    "spearmanr(avgHonestyRatings, wins)\n",
    "spearmanr(avgHonestyRatings, losses)\n",
    "spearmanr(avgHonestyRatings, ties)\n",
    "\n",
    "# no correlation at all between game outcome and honesty rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank-based spearman's correlation game results (win/loss/tie) & honesty rating\n",
    "for i in range(len(RThonesty)):\n",
    "    cor = spearmanr(transGameResults[i], honestyRatings[i])\n",
    "    print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank-based spearman's correlation game results (win/loss/tie) & RT honesty rating\n",
    "for i in range(len(RThonesty)):\n",
    "    cor = spearmanr(transGameResults[i], RThonesty[i])\n",
    "    print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get individual correlates level of expectation violation\n",
    "\n",
    "for i in range(len(uuids)):\n",
    "    for index, mask in enumerate(masks):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"participant\", i)\n",
    "        print(transGameResults[i][mask])\n",
    "        print(np.array(RThonesty[i])[mask])\n",
    "        pcorr = pearsonr(transGameResults[i][mask], np.array(RThonesty[i])[mask])\n",
    "        scorr = spearmanr(transGameResults[i][mask], np.array(RThonesty[i])[mask])\n",
    "        print(index, \"pearson's r\", pcorr)\n",
    "        print(index, \"spearman's r\", scorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get individual correlates level of expectation violation\n",
    "\n",
    "for i in range(len(uuids)):\n",
    "    for index, mask in enumerate(masks):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"participant\", i)\n",
    "        rating = np.array(honestyRatings[i])[mask]\n",
    "        RT = np.array(RThonesty[i])[mask]\n",
    "        print(rating)\n",
    "        print(RT)\n",
    "        pcorr = pearsonr(rating, RT)\n",
    "        scorr = spearmanr(rating, RT)\n",
    "        print(index, \"pearson's r\", pcorr)\n",
    "        print(index, \"spearman's r\", scorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlate how much participant lied per level with average honesty rating for each level of expectation violation\n",
    "# i.e. r(lied, honestyrating) | exp_violation\n",
    "\n",
    "avg_rating = []\n",
    "avg_prop_lie = []\n",
    "\n",
    "for index, mask in enumerate(masks):\n",
    "    for i in range(len(uuids)):      \n",
    "        rating = np.array(honestyRatings[i])[mask]\n",
    "        lied = np.array(reportHonesty[i])[mask]\n",
    "        \n",
    "        mean_rating = sum(rating)/len(rating)\n",
    "        n_lied = (len(lied)-sum(lied))/len(lied)\n",
    "        \n",
    "        avg_rating.append(mean_rating)\n",
    "        avg_prop_lie.append(n_lied)\n",
    "#         print(\"=\" * 50)\n",
    "#         print(\"participant\", i)\n",
    "#         print(rating)\n",
    "#         print(lied)\n",
    "#         print(mean_rating)\n",
    "#         print(n_lied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[0:4], avg_prop_lie[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[5:9], avg_prop_lie[5:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[10:14], avg_prop_lie[10:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[15:19], avg_prop_lie[15:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[20:24], avg_prop_lie[20:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[25:29], avg_prop_lie[25:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[30:34], avg_prop_lie[30:34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[35:39], avg_prop_lie[35:39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect correlation  between exp_violation and avg_prop_lie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression or nlme for honestyrating <- result; lying <- result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
