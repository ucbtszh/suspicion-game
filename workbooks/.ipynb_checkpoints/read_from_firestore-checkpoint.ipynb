{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE: load trial info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/Sarah Zheng/dev/cards-deception/cards-game/src/assets/trials_40shuffled.json\", \"r\") as read_file:\n",
    "    trials_source = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = pd.DataFrame(trials_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add expectation violation levels\n",
    "\n",
    "trials['exp_violation'] = np.where(trials['outcome'] == -1, trials['outcome'] - trials['outcome'] * (trials['n_red']/5), \\\n",
    "                                   trials['outcome'] - trials['outcome'] * (5-trials['n_red'])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_red</th>\n",
       "      <th>outcome</th>\n",
       "      <th>exp_violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_red  outcome  exp_violation\n",
       "0      1       -1           -0.8\n",
       "1      3       -1           -0.4\n",
       "2      2        1            0.4\n",
       "3      1       -1           -0.8\n",
       "4      2       -1           -0.6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=trials.exp_violation, legend=False).set(xlabel=\"trial\", ylabel=\"expectation violation at each trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial-to-trial change in expectation violation\n",
    "exp_violation_gradient = trials.exp_violation.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=exp_violation_gradient, legend=False).set(xlabel=\"trial\", ylabel=\"change in expectation violation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative sum of expectation violation over trials\n",
    "sns.lineplot(data=trials.exp_violation.cumsum(), legend=False).set(xlabel=\"trial\", ylabel=\"cumulative range in expectation violation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks for each level of expectation violation\n",
    "mask_min08 = trials[trials.exp_violation == '-0.8'].index.tolist()\n",
    "mask_min06 = trials[trials.exp_violation == '-0.6'].index.tolist()\n",
    "mask_min04 = trials[trials.exp_violation == '-0.4'].index.tolist()\n",
    "mask_min02 = trials[trials.exp_violation == '-0.2'].index.tolist()\n",
    "\n",
    "mask_08 = trials[trials.exp_violation == '0.8'].index.tolist()\n",
    "mask_06 = trials[trials.exp_violation == '0.6'].index.tolist()\n",
    "mask_04 = trials[trials.exp_violation == '0.4'].index.tolist()\n",
    "mask_02 = trials[trials.exp_violation == '0.2'].index.tolist()\n",
    "\n",
    "masks = [mask_min08, mask_min06, mask_min04, mask_min02, mask_02, mask_04, mask_06, mask_08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexes of trials by # red cards = states\n",
    "idx_n_red = []\n",
    "\n",
    "for i in set(trials.n_red):\n",
    "    idxs = trials[trials.n_red == i].index.tolist()\n",
    "    print(\"#red cards:\", i, \"at\", idxs)\n",
    "    idx_n_red.append(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get pilot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pilot uuids\n",
    "uuids = ['5fe200ac3803bbcff9845530', \n",
    "         '5ff5c230bce45b062a2a06b2', \n",
    "         '5f1b05a8703c380d1393bb27', \n",
    "         '5b424267e8815c0001777d94', \n",
    "         '604294f7bacd9a1a46cf4da5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "\n",
    "# Use a service account\n",
    "cred = credentials.Certificate('D:/Behavioural cybersecurity/project-notes/cards-dd-game-61b9b7f26a6e.json')\n",
    "firebase_admin.initialize_app(cred)\n",
    "\n",
    "db = firestore.client()\n",
    "\n",
    "subjects_ref = db.collection(u'subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses query reference\n",
    "refs = []\n",
    "for uuid in uuids:\n",
    "    refs.append(subjects_ref.document(uuid).collection(u'responses').document('main_responses').get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographics survey query reference\n",
    "refs_demographics = []\n",
    "for uuid in uuids:\n",
    "    refs_demographics.append(subjects_ref.document(uuid).collection(u'responses').document('demographics').get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# card values\n",
    "transdict = {'blue': 1, 'red': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# literal card values\n",
    "transdict2 = {1: 'blue', -1: 'red'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result values\n",
    "results_transdict = {'tie': 0.5, 'loss': 1, 'win': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch responses from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameResponses(object):\n",
    "    def __init__(self, randomPick, randomPickColour, reportColour, RTreport, honestyRating, RThonesty,\n",
    "                 results, catchRating, RTcatch):\n",
    "        self.randomPick = randomPick\n",
    "        self.randomPickColour = randomPickColour\n",
    "        self.reportColour = reportColour\n",
    "        self.honestyRating = honestyRating\n",
    "        self.catchRating = catchRating\n",
    "        self.RThonesty = RThonesty\n",
    "        self.RTreport = RTreport\n",
    "        self.RTcatch = RTcatch\n",
    "        self.results = results\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(source):\n",
    "        gresponse = GameResponses(source[u'randomPick'], source[u'randomPickColour'], source[u'reportColour'],\n",
    "                                  source[u'RTreport'], source[u'honestyRating'], source[u'RThonesty'], \n",
    "                                  source[u'results'], source[u'catchRating'], source[u'RTcatch'])\n",
    "\n",
    "        return gresponse\n",
    "\n",
    "    def to_dict(self):\n",
    "        dest = {\n",
    "            u'randomPick': self.randomPick,\n",
    "            u'randomPickColour': self.randomPickColour,\n",
    "            u'reportColour': self.reportColour,\n",
    "            u'RTreport': self.RTreport,\n",
    "            u'honestyRating': self.honestyRating,\n",
    "            u'RThonesty': self.RThonesty,\n",
    "            u'results': self.results,\n",
    "            u'catchRating': self.catchRating,\n",
    "            u'RTcatch': self.catch\n",
    "        }\n",
    "\n",
    "        return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Demographics(object):\n",
    "    def __init__(self, age, catch, edlev, gender, twin):\n",
    "        self.age = age\n",
    "        self.catch = catch\n",
    "        self.edlev = edlev\n",
    "        self.gender = gender\n",
    "        self.twin = twin\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(source):\n",
    "        demos = Demographics(source[u'age'], source[u'catch'], source[u'edlev'],\n",
    "                                  source[u'gender'], source[u'twin'])\n",
    "\n",
    "        return demos\n",
    "\n",
    "    def to_dict(self):\n",
    "        dest = {\n",
    "            u'age': self.age,\n",
    "            u'catch': self.catch,\n",
    "            u'edlev': self.edlev,\n",
    "            u'gender': self.gender,\n",
    "            u'twin': self.twin\n",
    "        }\n",
    "\n",
    "        return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main game response data\n",
    "honestyRatings = []\n",
    "catchRating = []\n",
    "reportHonesty = []\n",
    "gameResults = []\n",
    "transGameResults = []\n",
    "RThonesty = []\n",
    "RTreport = []\n",
    "randomPickColour = []\n",
    "reportColour = []\n",
    "\n",
    "for index, subject in enumerate(refs):\n",
    "    responses = GameResponses.from_dict(subject.to_dict())\n",
    "    \n",
    "    ratings = [int(numeric_string) for numeric_string in responses.honestyRating]\n",
    "    honestyRatings.append(ratings)\n",
    "    \n",
    "    catch = [int(numeric_string) for numeric_string in responses.catchRating]\n",
    "    catchRating.append(catch)\n",
    "    \n",
    "    ifLied = np.array([transdict[x] for x in responses.randomPickColour]) == responses.reportColour\n",
    "    reportHonesty.append(ifLied)\n",
    "    \n",
    "    gameResults.append(responses.results)\n",
    "    \n",
    "    transResults = np.array([results_transdict[x] for x in responses.results]) \n",
    "    transGameResults.append(transResults)\n",
    "    \n",
    "    RThonesty.append(responses.RThonesty)\n",
    "    RTreport.append(responses.RTreport)\n",
    "    \n",
    "    randomPickColour.append([x for x in responses.randomPickColour])\n",
    "    reportColour.append([x for x in responses.reportColour])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check when subjects lied\n",
    "\n",
    "lying_overview = []\n",
    "lying_counts = []\n",
    "lying_probable = []\n",
    "\n",
    "for i in range(len(uuids)):\n",
    "    lie = 1 - reportHonesty[i]\n",
    "    tmp_sheet = pd.DataFrame({'random_pick_colour': [transdict[x] for x in randomPickColour[i]], \n",
    "                              'reported_colour': reportColour[i], \n",
    "                              'lied': lie})\n",
    "    tmp_sheet = trials.join(tmp_sheet)\n",
    "    tmp_sheet['lied_colour_probability'] = np.where(tmp_sheet['reported_colour']==-1, tmp_sheet['n_red']/5, (5-tmp_sheet['n_red'])/5)\n",
    "    tmp_sheet['lied_colour_probability'] = np.where(tmp_sheet['lied']==1, tmp_sheet['lied_colour_probability'], 0)\n",
    "    lying_overview.append(tmp_sheet)\n",
    "    \n",
    "    lie_blue = sum((tmp_sheet.lied == 1) & (tmp_sheet.reported_colour == 1))\n",
    "    lie_red = sum((tmp_sheet.lied == 1) & (tmp_sheet.reported_colour == -1))\n",
    "    lying_counts.append({'count_lie_blue': lie_blue, 'count_lie_red': lie_red, 'average_honesty_rating': np.array(honestyRatings[i]).mean()})\n",
    "    \n",
    "    lying_probable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "lying_counts = pd.DataFrame(lying_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_red</th>\n",
       "      <th>outcome</th>\n",
       "      <th>exp_violation</th>\n",
       "      <th>random_pick_colour</th>\n",
       "      <th>reported_colour</th>\n",
       "      <th>lied</th>\n",
       "      <th>lied_colour_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_red  outcome  exp_violation  random_pick_colour  reported_colour  lied  \\\n",
       "0      1       -1           -0.8                   1               -1     1   \n",
       "1      3       -1           -0.4                  -1                1     1   \n",
       "2      2        1            0.4                   1               -1     1   \n",
       "3      1       -1           -0.8                  -1               -1     0   \n",
       "4      2       -1           -0.6                  -1                1     1   \n",
       "\n",
       "   lied_colour_probability  \n",
       "0                      0.2  \n",
       "1                      0.4  \n",
       "2                      0.4  \n",
       "3                      0.0  \n",
       "4                      0.6  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lying_overview[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x23cc2ee45b0>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 438.5x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(data=lying_overview[3], x='lied_colour_probability', hue='reported_colour', multiple='stack').set(xlim=(0.15,0.9), ylim=(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, pearsonr, ttest_ind, wilcoxon, ranksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7891848733011686, 0.11244825052684987)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(lying_counts['average_honesty_rating'], lying_counts['count_lie_blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6149967904317033, 0.2695901711383851)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(lying_counts['average_honesty_rating'], lying_counts['count_lie_red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_lie_blue</th>\n",
       "      <th>count_lie_red</th>\n",
       "      <th>average_honesty_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>3.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_lie_blue  count_lie_red  average_honesty_rating\n",
       "0              10             12                   3.175\n",
       "1              12              2                   3.225\n",
       "2               0              0                   5.325\n",
       "3               2              2                   3.425\n",
       "4               1              0                   4.700"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lying_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=0.4177863742936748, pvalue=0.6761033140231469)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(lying_counts.count_lie_blue, lying_counts.count_lie_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "writer = pd.ExcelWriter('pilot1_lying_per_subject.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for i, df in enumerate(lying_overview):\n",
    "    df.to_excel(writer, sheet_name='subject'+str(i))\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing honesty ratings -> suspicion ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [honesty rating] normalize honesty ratings to range [0;1] and reverse-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_honesty_ratings = []\n",
    "\n",
    "for ratings in honestyRatings:\n",
    "    normed = [(v/6) for v in ratings]\n",
    "    normalized_honesty_ratings.append(normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse-code honesty ratings to reflect suspicion\n",
    "import numexpr\n",
    "suspicionRating = numexpr.evaluate('(6 - honestyRatings)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_suspicion_ratings = numexpr.evaluate('(1 - normalized_honesty_ratings)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_suspicion_ratings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [honesty rating] binarize reversed-coded honesty ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_suspicion_ratings = []\n",
    "\n",
    "for ratings in suspicionRating:\n",
    "    binary = [1 if v >= 4 else 0 for v in ratings]\n",
    "    binary_suspicion_ratings.append(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_suspicion_ratings[1][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [honesty rating] z-scored raw reverse-coded honesty ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 1: get pooled mean and SD stats from all participants\n",
    "bin_mean_HRs = []\n",
    "bin_sd_HRs = []\n",
    "\n",
    "for rating in suspicionRating:\n",
    "    bin_mean_HRs.append(sum(rating)/len(rating))\n",
    "    bin_sd_HRs.append(np.array(rating).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean_HRs = np.array(bin_mean_HRs).mean()\n",
    "z_sd_HRs = np.array(bin_mean_HRs).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-scored suspicion ratings compared cross-participants\n",
    "z_suspicion_ratings = []\n",
    "\n",
    "for rating in suspicionRating:\n",
    "    z_rating = [(x-z_mean_HRs)/z_sd_HRs for x in rating]\n",
    "    z_suspicion_ratings.append(z_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sd_HRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 2: z-score each participant's ratings -> takes away inter-subject variance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [honesty rating] log transform normalized reverse-coded honesty ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_log_suspicionrating = [np.log(v+1) for v in normalized_suspicion_ratings[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=tmp_log_suspicionrating, legend=False).set(xlabel=\"trial\", ylabel=\"log normalized honesty rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial inspection plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot honesty ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(honestyRatings)):\n",
    "    sns.lineplot(data=honestyRatings[i], legend=False).set(xlabel=\"trial\", ylabel=\"honesty rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(honestyRatings)):\n",
    "    sns.lineplot(data=normalized_suspicion_ratings[i], legend=False).set(xlabel=\"trial\", ylabel=\"suspicion rating (reversed honesty)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject that lied most\n",
    "sns.lineplot(data=honestyRatings[0], legend=False)\n",
    "sns.lineplot(data=gameResults[0], legend=False).set(xlabel=\"trial\", ylabel=\"game result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject that lied most\n",
    "sns.lineplot(data=honestyRatings[0], legend=False)\n",
    "sns.lineplot(data=trials.exp_violation, legend=False).set(xlabel=\"trial\", ylabel=\"expectation violation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_tmp = exp_violation_gradient.cumsum().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject that lied most\n",
    "sns.lineplot(data=honestyRatings[0], legend=False)\n",
    "sns.lineplot(data=ev_tmp, legend=False).set(xlabel=\"trial\", ylabel=\"cumulative expectation violation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject that lied most\n",
    "sns.lineplot(data=honestyRatings[0], legend=False)\n",
    "sns.lineplot(data=exp_violation_gradient, legend=False).set(xlabel=\"trial\", ylabel=\"change in expectation violation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# subject that lied least\n",
    "sns.lineplot(data=honestyRatings[2], legend=True)\n",
    "sns.lineplot(data=gameResults[2], legend=True).set(xlabel=\"trial\", ylabel=\"game result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save specific subject data to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_subject(i):\n",
    "    dict_subject = {\n",
    "        \"trial\": range(1,41),\n",
    "        \"n_red\": trials.n_red,\n",
    "        \"n_blue\": 5-trials.n_red,\n",
    "        \"opponent_report\": trials.outcome,\n",
    "        \"expectation_violation\": trials.exp_violation,\n",
    "        \"random_pick_colour\": [transdict[x] for x in randomPickColour[i]],\n",
    "        \"reported_pick_colour\": reportColour[i],\n",
    "        \"honest_card_report\": reportHonesty[i],\n",
    "        \"trial_result\": transGameResults[i],\n",
    "        \"honesty_rating\": honestyRatings[i],\n",
    "        \"RT_honesty\": RThonesty[i],\n",
    "        \"normalized_reversed_honesty_rating\": normalized_suspicion_ratings[i]\n",
    "    }\n",
    "\n",
    "    df_subject = pd.DataFrame(dict_subject)\n",
    "    df_subject['surprise'] = abs(df_subject['expectation_violation'])\n",
    "    df_subject['plot_partner_report'] = 1.1\n",
    "    df_subject['plot_subject_report'] = -0.1\n",
    "    df_subject['plot_random_pick_colour'] = -0.15\n",
    "    df_subject['lied'] = df_subject['random_pick_colour'] != df_subject['reported_pick_colour']\n",
    "    df_subject['colour_incongruent'] = df_subject['reported_pick_colour'] != df_subject['opponent_report']\n",
    "    df_subject['colour_congruent'] = df_subject['reported_pick_colour'] == df_subject['opponent_report']\n",
    "    \n",
    "    df_subject['partner_result'] = [1 if x == 0 else 0 if x == 1 else x for x in df_subject.trial_result.values]\n",
    "    return df_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers2 = {\"loss\": \"X\", \"win\": \"o\", \"tie\": \"D\"}\n",
    "colors = {-1: 'red', 1: 'blue'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_plot_subject(df_subject, i):\n",
    "    sns.set(rc={'figure.figsize':(16,7)})\n",
    "    p = sns.lineplot(data=df_subject, x='trial', y='normalized_reversed_honesty_rating')\n",
    "    p.set(ylim=(-0.25,1.2), xlim=(0, 41))\n",
    "    p.set_ylabel('actual normalized reversed honesty ratings')\n",
    "    p.set_title('Cards game progression subject '+str(i+1))\n",
    "\n",
    "    p2 = sns.lineplot(data=df_subject, x='trial', y='surprise')\n",
    "#     p2 = sns.lineplot(data=trials, x='trial', y='normed_signed_colour_count')\n",
    "    s = sns.scatterplot(data=df_subject, x='trial',y='plot_random_pick_colour', c=[colors[x] for x in df_subject.random_pick_colour])\n",
    "    s1 = sns.scatterplot(data=df_subject, x='trial',y='plot_subject_report', c=[colors[x] for x in df_subject.reported_pick_colour])\n",
    "    s2 = sns.scatterplot(data=df_subject, x='trial',y='plot_partner_report', c=[colors[x] for x in df_subject.opponent_report])\n",
    "    s3 = sns.scatterplot(data=df_subject, x='trial',y='trial_result', markers=markers2, style=gameResults[i])\n",
    "    s3.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    s4 = sns.scatterplot(data=df_subject, x='trial', y=[-0.2 if x else None for x in df_subject.lied])\n",
    "    s5 = sns.scatterplot(data=df_subject, x='trial', y=[1.15 if x == True else None for x in df_subject.colour_congruent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "df_subject = create_df_subject(i)\n",
    "create_plot_subject(df_subject, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearmanr(df_subject.lied, df_subject.normalized_reversed_honesty_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_subject.lied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((df_subject.random_pick_colour == -1) & (df_subject.reported_pick_colour == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(df_subject.colour_incongruent, df_subject.normalized_reversed_honesty_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(abs(df_subject.expectation_violation), df_subject.normalized_reversed_honesty_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(abs(df_subject.expectation_violation), df_subject.lied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(df_subject.expectation_violation, df_subject.lied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(df_subject.normalized_reversed_honesty_rating, df_subject.lied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subject.to_csv('pilot_subject1.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export honesty ratings to .csv file\n",
    "# import csv\n",
    "# with open('pilot_honestyratings.csv', 'w') as csvfile:\n",
    "#     csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "#     csvwriter.writerows(honestyRatings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [CONTROL] attention check questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographics data\n",
    "edlev = []\n",
    "catch = []\n",
    "\n",
    "for subject in refs_demographics:\n",
    "    demos = Demographics.from_dict(subject.to_dict())\n",
    "    edlev.append(demos.edlev)\n",
    "    catch.append(demos.catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-game attention checks\n",
    "catchRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_feedback = []\n",
    "\n",
    "for uuid in uuids:\n",
    "    refs_feedback.append(subjects_ref.document(uuid).collection(u'responses').document('feedback').get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feedback in refs_feedback:\n",
    "    print(feedback.to_dict()['studyPurpose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RT honesty ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RThonesty_stats = pd.DataFrame(columns=['mean', 'sd', 'min', 'max'])\n",
    "\n",
    "for id in RThonesty:\n",
    "    mean = sum(id)/len(id)\n",
    "    sd = np.array(id).std()\n",
    "    vmin = min(id)\n",
    "    vmax = max(id)\n",
    "    tmp = {'mean': mean, 'sd': sd, 'min': vmin, 'max': vmax}\n",
    "    print(tmp)\n",
    "    RThonesty_stats.append(tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter estimation - grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import *\n",
    "from specs import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process trials\n",
    "from specs import Trial\n",
    "\n",
    "Trials = []\n",
    "for trial in trials_source:\n",
    "    Trials.append(Trial(n_red=trial['n_red'], outcome=trial['outcome']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [model 1] using unsigned suspicion formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid space\n",
    "alpha = np.linspace(0.1, 1, 10)\n",
    "s0 = [-1, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 1]\n",
    "\n",
    "params = (list(product(alpha, s0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[142]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search param value fit\n",
    "best_param_sv_rating = []\n",
    "all_residuals_sv_rating = []\n",
    "all_R2_sv_rating = []\n",
    "best_param_sv_rating_SSE = []\n",
    "best_param_sv_rating_R2 = []\n",
    "sklearn_r2 = []\n",
    "\n",
    "for p, data in enumerate(normalized_suspicion_ratings):\n",
    "    suspicion_ratings = np.array(data)\n",
    "    SStot_sv_rating = np.sum((suspicion_ratings-suspicion_ratings.mean())**2)\n",
    "    SSres_sv_rating = []\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        print(\"iterating parameter set\", i)\n",
    "        g = Game(Trials, Player(alpha=param[0], pre_suspicion=param[1]))\n",
    "        g.simulate_unsigned(verbose=False, save=False, add_noise=False)\n",
    "\n",
    "#         sim_sv_rating = g.unsigned_suspicion_to_honesty_rating()\n",
    "        sim_sv_rating = g.unsigned_expectation_violation\n",
    "        residuals_sv_rating = np.sum((suspicion_ratings-sim_sv_rating)**2)\n",
    "        SSres_sv_rating.append(residuals_sv_rating)\n",
    "        \n",
    "        sklearn_r2.append(r2_score(data, sim_sv_rating))\n",
    "        \n",
    "    R2_sv_rating = 1 - np.divide(SSres_sv_rating, SStot_sv_rating)\n",
    "    all_residuals_sv_rating.append(SSres_sv_rating)\n",
    "    all_R2_sv_rating.append(R2_sv_rating)\n",
    "    \n",
    "    best_idx_sv_rating = np.array(SSres_sv_rating).argmin()\n",
    "    best_param_sv_rating.append(params[best_idx_sv_rating])\n",
    "    best_param_sv_rating_SSE.append(np.array(SSres_sv_rating).min())\n",
    "    best_param_sv_rating_R2.append(R2_sv_rating[best_idx_sv_rating])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_sv_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(params) #* len(uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_param_sv_rating_R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(sklearn_r2[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_residuals_sv_rating[0]).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[98]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [model 2] signed suspicion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter search space\n",
    "\n",
    "alpha = np.linspace(0.01,1,100)\n",
    "s0 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_suspicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_suspicion.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = generated_suspicion - generated_suspicion.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(t ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SStot_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = SSres_sv[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSres_sv = []\n",
    "generated_suspicion = normalized_suspicion_ratings[0]\n",
    "SStot_sv = np.sum((generated_suspicion - generated_suspicion.mean())**2)\n",
    "logs = []\n",
    "\n",
    "for i, param in enumerate(alpha):\n",
    "    g = Game(Trials, Player(alpha=param, pre_suspicion=s0))\n",
    "    g.simulate_signed(verbose=False, save=False, add_noise=False)\n",
    "\n",
    "    sv = np.array(g.suspicion_values)\n",
    "    print(\"model output\", sv)\n",
    "    sv = normalized_array(sv, s0)\n",
    "    print(\"normalized model output\", sv)\n",
    "    residuals_sv = np.sum((generated_suspicion-sv)**2)\n",
    "    SSres_sv.append(residuals_sv)\n",
    "    tmp = pd.DataFrame(g.sim_log)\n",
    "    tmp['normalized_estimated_suspicion'] = g.normalize_signed_suspicion()\n",
    "    tmp['normalized_actual_suspicion_ratings_subject1'] = generated_suspicion\n",
    "    logs.append(tmp)\n",
    "    \n",
    "R2_sv = 1 - np.divide(SSres_sv, SStot_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_sv.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha[76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "writer = pd.ExcelWriter('pilot_subject1_signed-suspicion-noprior_gridsearch.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for i, df in enumerate(logs):\n",
    "    df.to_excel(writer, sheet_name='fit_alpha_'+str(alpha[i]))\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({'alpha': alpha, 'SS_res': SSres_sv, 'SS_tot': [SStot_sv]*len(alpha), 'R_squared': R2_sv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv('pilot_subject1_signed-suspicion-noprior_gridsearch_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [model 3] using softmax probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_recovery_proba(x, params):\n",
    "    '''returns sum of squared residuals for each fitted parameter value combination, and best parameter values index'''\n",
    "    SSres = []\n",
    "    for param in params:\n",
    "        player = Player(alpha=param[0], beta=param[1])\n",
    "        g = Game(trials, player)\n",
    "        g.simulate(verbose=False)\n",
    "        ps = np.array(g.softmax_probabilities)\n",
    "        SSres.append(sum((x-ps)**2))\n",
    "    minimum_idx = np.where(SSres == np.amin(SSres))[0]\n",
    "    print(\"SSresiduals minimum index:\", minimum_idx)\n",
    "    for idx in minimum_idx:\n",
    "        print(\"BEST PARAMETER ESTIMATES:\", params[idx])\n",
    "    return np.array(SSres), minimum_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_best_estimations(x, params):\n",
    "    '''generates long dataframe with estimated and actual data'''\n",
    "    x = np.array(x)\n",
    "    SSres, best_params_idx = param_recovery_proba(x, params)\n",
    "    SStot = sum([(v-x.mean())**2 for v in x])\n",
    "    R2 = 1-SSres[best_params_idx]/SStot\n",
    "    \n",
    "    best_sim = Game(trials, Player(alpha=params[best_params_idx[0]][0], beta=params[best_params_idx[0]][1]))\n",
    "    best_sim.simulate(verbose=False)\n",
    "    best_estimated_proba = best_sim.softmax_probabilities\n",
    "    \n",
    "    df = pd.DataFrame({'value': best_estimated_proba, 'label': ['estimated'] * len(best_estimated_proba)})\n",
    "    df = df.append(pd.DataFrame({'value': x, 'label': ['actual'] * len(x)}), ignore_index=False)\n",
    "    print(\"SSres\", SSres[best_params_idx])\n",
    "    print(\"SStot\", SStot)\n",
    "    print(\"R squared:\", R2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_best_estimations(normalized_suspicion_ratings[0], params)\n",
    "sns.lineplot(data=df, x=df.index, y='value', hue='label').set(xlabel=\"trial\", ylabel=\"p(deception)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_best_estimations(transGameResults[4], params)\n",
    "sns.lineplot(data=df, x=df.index, y='value', hue='label').set(xlabel=\"trial win/loss/tie\", ylabel=\"p(deception)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [model 4] fit simple colour report counts tracking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add partner colour report outcome counts to trial\n",
    "\n",
    "count_red = abs(trials.outcome[lambda x: x==-1].cumsum())\n",
    "count_blue = abs(trials.outcome[lambda x: x==1].cumsum())\n",
    "\n",
    "trials['colour_count'] = count_red.append(count_blue).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['colour_count_prop'] = trials.colour_count/len(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approach 1: track proportion of reported colours\n",
    "\n",
    "prop_trial_red = []\n",
    "prop_trial_blue = []\n",
    "\n",
    "t_burnin = 0 # how many trials in memory\n",
    "\n",
    "for i in range(len(trials)):\n",
    "    tmp = trials.outcome[:i]\n",
    "    n_red = abs(tmp[lambda x: x==-1].cumsum())\n",
    "    n_blue = abs(tmp[lambda x: x==1].cumsum())\n",
    "    prop_trial_red.append(n_red/i)\n",
    "    prop_trial_blue.append(n_blue/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_trial_red[2].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approach 2: track # same consecutive colour reports since the reported colour changed\n",
    "\n",
    "track_freq = [1] * len(trials)\n",
    "\n",
    "for i, outcome in enumerate(trials.outcome.values):\n",
    "    if (i==0):\n",
    "        continue\n",
    "    if (i>0):\n",
    "        if (outcome != trials.outcome.values[i-1]):\n",
    "            continue\n",
    "        if (outcome == trials.outcome.values[i-1]):\n",
    "            track_freq[i] = track_freq[i-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['trial'] = trials.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['n_consec_colour'] = track_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['signed_n_consec_colour'] = trials['n_consec_colour'] * trials['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (trials.signed_n_consec_colour + 5)/9\n",
    "trials['normed_signed_colour_count'] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_count_tracker(data, params):\n",
    "    data = np.array(data)\n",
    "    ss_tot = np.sum((data - data.mean())**2)\n",
    "    ss_res = []\n",
    "    for param in params:\n",
    "        alpha = param[0]\n",
    "        prior = param[1]\n",
    "        estimated = np.array(trials.normed_signed_colour_count * alpha + prior)\n",
    "        ss_res.append(np.sum((data - estimated)**2))\n",
    "    r2 = 1 - np.divide(ss_res, ss_tot)\n",
    "    best_params = params[np.array(r2).argmax()]\n",
    "    return best_params, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid space\n",
    "alpha = np.linspace(0.1, 1, 10)\n",
    "prior = [-0.2, -0.1, 0, 0.1, 0.2]\n",
    "\n",
    "params = (list(product(alpha, prior)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, r2 = fit_count_tracker(normalized_suspicion_ratings[1], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, r2 = fit_count_tracker(normalized_suspicion_ratings[4], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=trials.normed_signed_colour_count * 0.8 - 0.2)\n",
    "sns.lineplot(data=normalized_suspicion_ratings[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overall analyses across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct RT report cumulative values\n",
    "rtreport0_tmp = pd.Series(RTreport[0]).diff().fillna(RTreport[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(rtreport0_tmp, honestyRatings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average honesty rating and SD\n",
    "avgHonestyRatings = []\n",
    "sdHonestyRatings = []\n",
    "\n",
    "for index, ratings in enumerate(honestyRatings):\n",
    "#     print(index, ratings)\n",
    "    ratingsInt = [int(numeric_string) for numeric_string in ratings]\n",
    "    mean = sum(ratingsInt)/len(ratingsInt)\n",
    "    avgHonestyRatings.append(mean)\n",
    "    sd = np.std(ratingsInt)\n",
    "    sdHonestyRatings.append(sd)\n",
    "    print(\"average honesty rating\", mean, \"std\", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgHonestyRatings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdHonestyRatings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propLied = []\n",
    "\n",
    "for index, report in enumerate(reportHonesty):\n",
    "#     print(index, report)\n",
    "    lieProportion = (len(report) - sum(report)) / len(report)\n",
    "    print(lieProportion)\n",
    "    propLied.append(lieProportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate pearson's correlation proportion the participant lied and average honesty rating\n",
    "pearsonr(propLied, avgHonestyRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(propLied, avgHonestyRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate game results per participant\n",
    "wins = []\n",
    "losses = []\n",
    "ties = []\n",
    "\n",
    "for result in gameResults:\n",
    "    unique, counts = np.unique(result, return_counts=True)\n",
    "    outcome = dict(zip(unique, counts))\n",
    "    wins.append(outcome['win'])\n",
    "    losses.append(outcome['loss'])\n",
    "    ties.append(outcome['tie'])\n",
    "#     print(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate spearman's correlation game results (win/loss/tie) & average honesty rating\n",
    "spearmanr(avgHonestyRatings, wins)\n",
    "spearmanr(avgHonestyRatings, losses)\n",
    "spearmanr(avgHonestyRatings, ties)\n",
    "\n",
    "# no correlation at all between game outcome and honesty rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank-based spearman's correlation game results (win/loss/tie) & honesty rating\n",
    "for i in range(len(RThonesty)):\n",
    "    cor = spearmanr(transGameResults[i], honestyRatings[i])\n",
    "    print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank-based spearman's correlation game results (win/loss/tie) & RT honesty rating\n",
    "for i in range(len(RThonesty)):\n",
    "    cor = spearmanr(transGameResults[i], RThonesty[i])\n",
    "    print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get individual correlates level of expectation violation\n",
    "\n",
    "for i in range(len(uuids)):\n",
    "    for index, mask in enumerate(masks):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"participant\", i)\n",
    "        print(transGameResults[i][mask])\n",
    "        print(np.array(RThonesty[i])[mask])\n",
    "        pcorr = pearsonr(transGameResults[i][mask], np.array(RThonesty[i])[mask])\n",
    "        scorr = spearmanr(transGameResults[i][mask], np.array(RThonesty[i])[mask])\n",
    "        print(index, \"pearson's r\", pcorr)\n",
    "        print(index, \"spearman's r\", scorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get individual correlates level of expectation violation\n",
    "\n",
    "for i in range(len(uuids)):\n",
    "    for index, mask in enumerate(masks):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"participant\", i)\n",
    "        rating = np.array(honestyRatings[i])[mask]\n",
    "        RT = np.array(RThonesty[i])[mask]\n",
    "        print(rating)\n",
    "        print(RT)\n",
    "        pcorr = pearsonr(rating, RT)\n",
    "        scorr = spearmanr(rating, RT)\n",
    "        print(index, \"pearson's r\", pcorr)\n",
    "        print(index, \"spearman's r\", scorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlate how much participant lied per level with average honesty rating for each level of expectation violation\n",
    "# i.e. r(lied, honestyrating) | exp_violation\n",
    "\n",
    "avg_rating = []\n",
    "avg_prop_lie = []\n",
    "\n",
    "for index, mask in enumerate(masks):\n",
    "    for i in range(len(uuids)):      \n",
    "        rating = np.array(honestyRatings[i])[mask]\n",
    "        lied = np.array(reportHonesty[i])[mask]\n",
    "        \n",
    "        mean_rating = sum(rating)/len(rating)\n",
    "        n_lied = (len(lied)-sum(lied))/len(lied)\n",
    "        \n",
    "        avg_rating.append(mean_rating)\n",
    "        avg_prop_lie.append(n_lied)\n",
    "#         print(\"=\" * 50)\n",
    "#         print(\"participant\", i)\n",
    "#         print(rating)\n",
    "#         print(lied)\n",
    "#         print(mean_rating)\n",
    "#         print(n_lied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[0:4], avg_prop_lie[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[5:9], avg_prop_lie[5:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[10:14], avg_prop_lie[10:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[15:19], avg_prop_lie[15:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[20:24], avg_prop_lie[20:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[25:29], avg_prop_lie[25:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[30:34], avg_prop_lie[30:34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(avg_rating[35:39], avg_prop_lie[35:39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect correlation  between exp_violation and avg_prop_lie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression or nlme for honestyrating <- result; lying <- result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
